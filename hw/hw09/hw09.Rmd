---
title: "Assignment 09"
author: "Your name and student ID"
date: "Today's date"
output:
  pdf_document: default
---


### Instructions 
* Solutions will be released on Tuesday, August 10
* This semester, homework assignments are for practice only and will not be turned in for marks.

```{r, warning=F, message=F, echo=F}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(ggrepel)
```


Helpful hints:

- Every function you need to use was taught during lecture! So you may need to 
revisit the lecture code to help you along by opening the relevant files on Datahub. 
Alternatively, you may wish to view the code in the condensed PDFs posted 
on the course website. Good luck!

- Knit your file early and often to minimize knitting errors! If you copy and 
paste code for the slides, you are bound to get an error that is hard to 
diagnose. Typing out the code is the way to smooth knitting! We recommend 
knitting your file each time after you write a few sentences/add a new code 
chunk, so you can detect the source of knitting errors more easily. This will 
save you and the GSIs from frustration! 

\newpage


### Section 1: Abstract interpretation [5 points]
Read the following abstract and answer the questions that follow. 
J Asthma. 2018 Oct 11:1-12. doi: 10.1080/02770903.2018.1508471. [Epub ahead of print]
Impact of scenario based training on asthma first aid knowledge and skills in school staff: an open label, three-arm, parallel-group repeated measures study.
Luckie K1, Saini B1, Soo YYB1,2, Kritikos V1,3, Charles Collins JB1, Jane Moles R1.

OBJECTIVE:
To test the hypothesis that scenario-based skills training is more effective than knowledge training alone in improving the asthma first aid (AFA) skills of school personnel. Education developed specifically for non-primary caregivers such as school staff is vital to minimize the risk of mortality associated with asthma.

METHODS:
Schools were allocated to one of three arms to compare AFA knowledge and AFA skills. Arm 1 underwent conventional asthma training, arm 2 underwent scenario-based training and arm 3 had a combination of the two. Conventional asthma training involved a didactic oral presentation. The scenario-based skills training required the participant to describe and demonstrate how they would manage a child having a severe exacerbation of asthma using equipment provided. Follow-up occurred at 3 weeks post baseline and again between 3-7 months after the first training/education visit.

RESULTS:
Nineteen primary schools (204 participants) were recruited. One-way ANOVA and Bonferroni Post-Hoc Tests showed there was a significant difference in AFA skills scores between the study arms who underwent scenario-based training; arms 2 and 3 (91.5% and 91.1%) and arm 1 who underwent conventional asthma training (77.3%) (p<0.001). AFA knowledge improved significantly in all study arms with no differences between study arms. Improvements seen in both AFA knowledge and AFA skills were maintained over time.

CONCLUSIONS:
Scenario-based training was superior to conventional didactic asthma training for AFA skills acquisition and overall competency in the administration of AFA and should be included in future asthma training programs.

\newpage

1. [1 point]	Two methods of hypothesis testing (types of tests) are mentioned in the abstract.  What is the null hypothesis for each of these tests (please list in the order they are mentioned in the abstract?

_Type your answer here, replacing this text._


\newpage

2. [1 point] There are two outcomes of interest in this study.  For which **outcome** would you conclude that there is a significant difference between the training groups. 

_Type your answer here, replacing this text._


\newpage

3. [1 point] If you were a school administrator why might you choose the arm 3 training?

_Type your answer here, replacing this text._

\newpage

4. [1 point] List one question you might want to ask about the methods, sample or results that would help you interpret the findings of this study?

_Type your answer here, replacing this text._

\newpage

5. [1 point]  What is another test that could have been considered for these study data?

_Type your answer here, replacing this text._


\newpage

### Section 2: ANOVA and Tukey's HSD [6 points]
For this question we will use the data from the NHANES survey
`
```{r read, echo=FALSE}
# Read CSV into R
nhanes <- read_csv("data/nhanes.csv")
head(nhanes)

nhanes <- na.omit(nhanes) #remove rows with missing values
```

\newpage

6. [1 point] Generate the mean and standard deviations in a dataframe for blood lipid level "lbdldl" by Blood pressure group "bpcat". Use dplyr functions.

```{r}
p6 <- 'Your code here'
p6

# YOUR CODE HERE
```

```{r}
. = ottr::check("tests/p6.R")
```

\newpage

7. [1 point] Create a boxplot that helps you to visualize these data.

```{r}
p7 <- 'Your plot here'
p7

# YOUR CODE HERE
```

```{r}
. = ottr::check("tests/p7.R")
```

\newpage

8. [2 points]  Conduct an ANOVA with Tukey's HSD for these data. Assign your model to the variable `tukey`.

```{r, warning = F}
tukey <- 'Your code here'
# p8 <- tidy(tukey) #uncomment this line

# YOUR CODE HERE
```

```{r}
. = ottr::check("tests/p8.R")
```

\newpage

9. [1 point]  What are the null and alternative hypotheses for this test?

_Type your answer here, replacing this text._

\newpage

10. [1 point]  What do you conclude from your analysis?

_Type your answer here, replacing this text._

\newpage

### Section 3: Non-parametric [3 points]
You are testing the change in test scores following an intensive tutoring session.  
You have the following data from a small group of students each student is tested before and after the tutoring session.  
Each row represents one student.  

|Time 1   |Time 2|
|---------|------|
|65       |77    |
|87       |100   |
|77       |75    |
|90       |89    |
|70       |80    |
|84       |81    |
|92       |91    |
|83       |96    |
|85       |84    |
|91       |89    |
|68       |88    |
|72       |100   |
|81       |81    |
|---------|------|

```{r}
#this code makes a dataframe of the table you see above
test_scores <- tribble(
  ~time1, ~time2,
  65, 77,
  87, 100,
  77, 75,
  90, 89,
  70, 80,
  84, 81,
  92, 91,
  83, 96,
  85, 84,
  91, 89,
  68, 88,
  72, 100,
  81, 81)

```


11. [2 point] Calculate the appropriate non-paramentric test for these data by hand. Attach an image to show your work if you would like. Make sure to place the image in the `src` directory. Uncomment the line by deleting the pound sign. Report the p-value> Keep it as a decimal and round to 4 places.

```{r}
#knitr::include_graphics('src/path-to-file')
p11 <- 'YOUR P-VALUE HERE'
p11

# YOUR CODE HERE
```

```{r}
. = ottr::check("tests/p11.R")
```

\newpage

12. [1 point] Check your work using [insert your test].test() function in R. Keep your answer as a decimal rounded to 4 decimals. Report your p-value and save it to the variable p12.

```{r, warning=FALSE}
p12 <- 'Your p-value here'
p12

# YOUR CODE HERE
```

```{r}
. = ottr::check("tests/p12.R")
```

\newpage


### Section 4: Voting during the 1992 election [21 points]

This code loads in the data frame `counties`:

```{r}
load("data/A10_counties.sav")
```

These data are from the 1992 election and looks at the percent of votes cast (for each county) for the `democrat` (Bill Clinton), `republican` (George Bush), and independent presidential nominees (Ross `Perot`). 

Ideally, if you were interested in voting patterns, you might look at the relationship between individual characteristics and whether each individual voted Democrat or Republican. However, data like that is often hard to come by. The `counties` data provide data on 3141 counties. Use `View()` to examine these data briefly and read the labels corresponding to the variables. Note that Alaska is not included and that two other counties with populations = 0 have also been excluded.

As discussed in class we have the entire population (not just a sample), so strictly speaking we don't need to perform statistical inference. However, we might pretend this is a sample so that we can apply the techniques of inference and gain competence creating and interpeting a linear model.

\newpage

13. [2 points] Looking only at California, plot the relationship between the % of votes cast for the Democratic candidate (`democrat`) and the population density of the county (`pop.density`). Since we will only be using the counties from California, go ahead and subset the full `counties` dataset to only include observations from the state of CA. 

```{r}
counties_CA <- 'SUBSET DATA HERE'

p13 <- 'YOUR PLOT HERE'
p13

# YOUR CODE HERE
```

```{r}
. = ottr::check("tests/p13.R")
```

\newpage

14. [1 point] The above plot you made does not look very good. 
The distribution of population density is skewed right, 
with a few counties having much higher densities than the majority of counties.
To see which counties these are, we will use `geom_text_repel` from the library `ggrepel` (loaded at the top of this assignment). 

The template for using this function is: 
`geom_text_repel(aes(label = your_labelling_var))`. 
You will want to set the labeling variable to be the variable in the dataset containing the county names.

```{r}
p14 <- 'YOUR CODE HERE'
p14

```

```{r}
. = ottr::check("tests/p14.R")
```

The current issue with these data is that San Francisco 
(as you can now hopefully point out) 
has a much higher population density than other counties, and that generally there is a large right skew in the distribution 
of the population density variable. 

If we tried and fit a linear model to these data, it would not fit well-- because the relationship between population density and the response variable is not linear. However, this is the perfect situation to try transforming the x variable.

\newpage

15. [2 points] Try adding a log-transformed version of population density to the data frame and remake your plot using this new variable. Call this new variable `log_pop_density`. Keep the population labels. Also add a smoothed fitted line:

```{r}
# uncomment the line below by deleting the pound sign
# counties_CA <- 'Add new variable here'

p15 <- 'YOUR PLOT HERE'
p15

```

```{r}
. = ottr::check("tests/p15.R")
```
    

\newpage

16. [4 points] Describe the relationship between the (logged) population density and the response variable in terms of the shape, direction, strength, and outliers.
These are concepts from Chapter 3. Calculate the correlation (round to 4 decimals) to comment on one of these aspects.

```{r}
p16 <- 'CALCULATE THE CORRELATION HERE'
p16

```

```{r}
. = ottr::check("tests/p16.R")
```
_Type your answer here, replacing this text._

\newpage

17. [4 points] Run a linear model regression of the % votes cast for the democratic candidate as a function of the population density. Make sure you get the order of variables right in the `lm()` function! Use the `tidy()` function to show the slope and intercept estimates. Interpret the relationship between the logged population density and the response variable. (You can `View()` the data frame to make sure you are getting your units right by checking the descriptions in the labels for each variable). Use another function from `broom` show the r-squared. Report and interpret this value for the model.

```{r}
lm_CA <- 'YOUR MODEL HERE'

r.squared <- 'Report r-squared here. Leave as decimal and round to 2 places'

```

```{r}
. = ottr::check("tests/p17.R")
```
_Type your answer here, replacing this text._

\newpage

18. [4 points] Using the code learned in class, that was also shown in Lab 9, make the four plots to examine the assumptions.

```{r}
plot1 <- 'Code for scatterplot here'
plot1

plot2 <- 'Code for QQ plot here'
plot2

plot3 <- 'Code for Fitted vs. Residuals plot here'
plot3

plot4 <- 'Code for Amount explained plot here'
plot4

```

```{r}
. = ottr::check("tests/p18.R")
```

19. [4 points] Comment on each of the plots and conclude about which assumptions appear violated vs. not violated. Don't forget to comment on the one assumption that cannot be investigated using plots.

_Type your answer here, replacing this text._



