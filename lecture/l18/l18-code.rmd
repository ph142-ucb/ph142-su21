---
title: "Flavors of T"
output:
---

<!-- libraries -->
```{r,include=FALSE,purl=FALSE}
library(knitr) # for include_graphics() 
library(dplyr)
library(readr)
```

### Flavors of T  
Last lecture we introduced the T test - which relaxes some of the assumptions we needed for the Z test.
Both the Z test and the T test we have learned so far have compared a sample to a hypothesized mean.

- These are **one sample** tests, meaning we have one variable of interest, 
and we take one sample. We're interested in knowing how the one sample differs
from some null hypothesis ($H_0: \mu = 68mm$)

Today we will look at additional types of T test:

- Comparison of the means of two samples
- Comparison of two means from paired samples

## Comparing means from two populations

### Comparing two population means

In many types of public health studies, rather than comparing one sample to a hypothesize value, we are interested in comparing two samples from two different populations.
We want to shed light on the question: Do these two samples come from two groups
with different underlying means? Or,

$H_O: \mu_1-\mu_2=0$ vs. $H_a: \mu_1 -\mu_2 \neq 0$ (two-sided alternative)

### Comparing two population means

We like to compare two means! In public health, we like to:

- run randomized controlled trials where we compare a treated subgroup to a 
placebo group. Do their mean health outcomes differ?
- conduct observation studies where we have exposed and unexposed individuals.
Do their mean health outcomes differ?

This is covered in chapter 18 in your book.  Note that we are skipping around a bit.

### Comparing two population means, graphically

- Make two histograms, one for each sample
- Compare their shapes, centers (means or medians) and spreads (standard deviations)
- Could instead make two box plots and compare their medians and IQRs

### Example with heights from two different populations

Are the heights of US and Dutch-born men different?

```{r heights-men, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
library(tidyverse)
set.seed(123)
dutch <- rnorm(n = 100, mean = 183, sd = 7.4)
usa <- rnorm(n = 100, mean = 175.5, sd = 7.4)

height_data_wide = data.frame(usa, dutch)
height_data <- data.frame(height = c(dutch, usa), country = c(rep("Dutch", 100), rep("USA", 100)))

height_data %>% group_by(country) %>% summarise(sample_mean = mean(height), 
                                                sample_sd = sd(height), 
                                                length = length(height))
```

### Example with heights from two different populations
Plotting two distributions:

ggplot(height_data, aes(x = height)) + 
  geom_histogram(aes(fill = country), binwidth = 5, col = "black") +
  theme_minimal(base_size = 15) +
  facet_wrap(~country, nrow = 2)

### Example with heights from two different populations
```{r men2, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
ggplot(height_data, aes(x = height)) + 
  geom_histogram(aes(fill = country), binwidth = 5, col = "black") +
  theme_minimal(base_size = 15) +
  facet_wrap(~country, nrow = 2)
```

### Example with heights from two different populations
If we take out the last line of that code, the histograms will be plotted on one grid.  Notice what happens to the columns...
```{r men3, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
ggplot(height_data, aes(x = height)) + 
  geom_histogram(aes(fill = country), binwidth = 5, col = "black") +
  theme_minimal(base_size = 15)
```
### Example with heights from two different populations
Revisiting the code for box plots :

ggplot(height_data, aes(y = height)) + 
  geom_boxplot(aes(fill = country), col = "black") +
  theme_minimal(base_size = 15)
  
### Example with heights from two different populations  
```{r men4, fig.align='center', out.width="80%", warning=F, message=F, echo=F}
ggplot(height_data, aes(y = height)) + 
  geom_boxplot(aes(fill = country), col = "black") +
  theme_minimal(base_size = 15) 
```


## Two sample T test 

###Independent or Non-Independent samples?

An important requirement for using a basic t-test as a tool for this comparison, is that the observations from the two groups are independent from one another.  If there is a relationship between the two groups we will will use a different type of t-test, the paired t-test which we will introduce next lecture.

When we compare these two samples, we also assume that both populations were sampled in the same (random) way and that the measurement of the variable we are comparing was done in the same way.


### Conditions for inference comparing two means

- Both populations are **Normally distributed**. The means and standard 
deviations of the populations are unknown. In practice, it is enough that the 
distributions have similar shapes and that the data have no strong outliers.

### Notation

Notation for the population parameters 

| Population | Variable | Population mean | Population SD | 
|------------|----------|-----------------|---------------|
| 1          | $x_1$    | $\mu_1$         | $\sigma_1$    | 
| 2          | $x_2$    | $\mu_2$         | $\sigma_2$    | 

Notation for the sample statistics

| Population | Sample size | Sample mean | Sample SD | 
|------------|-------------|-------------|-----------|
| 1          | $n_1$       | $\bar{x_1}$ | $s_1$     | 
| 2          | $n_2$       | $\bar{x_2}$ | $s_2$     | 

To perform inference about the difference between $\mu_1-\mu_2$ between the 
means of the two populations, we start from the difference $\bar{x}_1-\bar{x}_2$
between the means of the two samples.

### Two-sample $t$ procedures

- With one-sample procedures, we had one $\bar{x}$ and we would draw the sampling
distribution for $\bar{x}$. It was centered at $\mu$ with standard error 
$\sigma/\sqrt{n}$

- With two-samples, we have two sample averages $\bar{x}_1$ and $\bar{x}_2$.

- What do we do?


### Example with heights from two different populations

Are the heights of US and Dutch-born men different?

$\bar{x}_{USA} = 174.7042$ and $\bar{x}_{D} = 183.6690$ so the difference is
$\bar{x}_{D} - \bar{x}_{USA} = 8.9648$

What would happen if we took another set of two samples of USA and Dutch-born
men? We would expect these sample means to change. We could draw an approximate
sampling distribution for the difference between these two means.

### The sampling distribution of $\bar{x}_1-\bar{x}_2$

The distribution of the difference between two independent random variables has 
a mean equal to the difference of their respective means and a variance equal to 
the sum of their respective variances. That is:

The mean of the sampling distribution for $\bar{x}_1-\bar{x}_2$ is $\mu_1-\mu_2$.

The standard deviation of the sampling distribution is: 
$\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$

Our *estimate* of the standard deviation of the sampling distribution is: 
$SE=\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$

### Recall the one sample t-test!

$$\frac{\bar{x}-\mu}{s/\sqrt{n}}$$

We need to generalize this by replacing each piece in the t-test by the 
calculations on the previous slide:

The **two-sample** t-test is therefore:

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{SE}$$

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$
The two-sample t statistics has approximately a $t$ distribution. The approximation
is accurate when both sample sizes are greater than or equal to 5.

### Degrees of freedom for the two-sample t-test...

is bananas.

$$df = \frac{(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2})^2}{\frac{1}{n_1-1}(\frac{s_1^2}{n_1})^2 + \frac{1}{n_2-1}(\frac{s_2^2}{n_2})^2}$$

Often this is approximated by assuming that the degrees of freedom is = $n_{1} - 1$ or $n_{2}-1$ whichever is smaller, 
or by making an assumption that the variance in the two samples is equal and approximating the df with $n_{1}+n_{2}-2$

We will NOT calculate df by hand - we will use R

### Hypothesis testing when you have two samples

$H_0: \mu_1-\mu_2=0$, obtain the **two-sample t-test** statistic

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$

where the test **p-value** is the probability, when $H_0$ is true, of getting a
test statistics $t$ at least as extreme in the direction of $H_a$ as that obtained, 
and is computed as the corresponding area under the $t$ distribution with the
appropriate degrees of freedom.


### Example, continued

Let R do the work for you: 

```{r}
t.test(height_data_wide %>% pull(usa), 
       height_data_wide %>% pull(dutch), 
       alternative = "two.sided")
```

### Example, continued
Note that `t.test` gives you both the t-test results (t-statistic (called "t" in 
the output), df, and p-value), as well as the 95% CI. We got both because we 
performed a two-sided test.


### Example:  Zika vaccine 2017 article
```{r zika, out.width="80%", fig.align='center', echo=FALSE}
knitr::include_graphics("zika.png")

```

### Example:  Zika vaccine
STATISTICAL ANALYSIS
The antibody-binding response that was assessed on ELISA is reported as the proportion of participants in whom an antibody response developed at a given time point and as the geometric mean titer (both with 95% confidence intervals). We used Fisher's exact test to determine positive response rates and Student's t-test to compare the magnitude of the log-transformed antibody response between the two dose groups and within individuals as the change from baseline. 

### Example:  Zika vaccine
```{r zika2, out.width="80%", fig.align='center' , echo=FALSE}
knitr::include_graphics("titer.png")

```


### Example: Transgenic chickens

Infection of chickens with the avian flu is a threat to both poultry production
and human health. A research team created transgenic chickens resistant to avian
flu infection. Could the modification affect the chicken in other ways? The 
researchers compared the hatching weights (in grams) of 45 transgenic chickens
and 54 independently selected commercial chickens of the same breed.

```{r, echo=FALSE}
transgenic <- c(38.8, 39.0, 39.7, 40.0, 40.8, 40.9, 41.0, 41.0, 41.0, 42.5, 42.6, 43.0,
                43.0, 43.4, 43.5, 43.5, 43.8, 44.4, 44.7, 44.7, 44.7, 45.3, 45.7, 45.8, 
                46.4, 46.5, 46.6, 46.7, 46.7, 46.8, 46.9, 47.1, 47.1, 47.1, 47.3, 47.6,
                47.7, 48.1, 48.3, 49.3, 49.3, 49.8, 50.3, 50.9, 52.1)

commercial <- c(36.7, 37.1, 38.9, 39.5, 39.5, 39.8, 40.0, 40.2, 40.3, 40.5, 40.5, 40.7,
                41.1, 41.2, 41.5, 41.5, 41.6, 41.6, 41.7, 42.4, 43.1, 43.3, 43.3, 43.4,
                43.7, 44.1, 44.2, 45.2, 45.3, 45.4, 46.0, 46.1, 46.4, 46.6, 46.6, 46.9, 
                47.3, 47.5, 48.1, 48.2, 48.4, 48.6, 49.0, 49.1, 49.3, 49.6, 50.1, 50.2,  
                50.4, 50.6, 52.2, 53.0, 55.5, 56.4)

chicken_data <- data.frame(weight = c(transgenic, commercial), 
                           type = c(rep("transgenic", 45), rep("commercial", 54)))
```

### Example: Transgenic chickens
```{r, fig.align='center', out.width="80%", echo=F}
ggplot(chicken_data, aes(y = weight)) + geom_boxplot(aes(fill = type)) + theme_minimal(base_size = 15)

ggplot(chicken_data, aes(x = weight)) + geom_histogram(aes(fill = type), binwidth = 3, col = "black") +
  facet_wrap(~type, nrow = 2) + theme_minimal(base_size = 15)
```

### Estimate the size of the difference between the two means

```{r}
means <- chicken_data %>% 
  group_by(type) %>% 
  summarise(mean_weight = mean(weight))

diff_means <- means[1, 2] - means[2, 2]
diff_means
```

The estimated mean difference is -0.153 grams.

### Estimate the standard error

$SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$

```{r}
chicken_stats <- chicken_data %>% 
  group_by(type) %>% 
  summarise(mean_weight = mean(weight),
            sd_weight = sd(weight), 
            n = length(weight)) 
```
 Use the output to calculate the SE:
 
$SE = \sqrt{\frac{4.568872^2}{54} + \frac{3.320836^2}{45}} = 0.7947528$

### Calculate the t-statistic

$$t=\frac{(\bar{x}_1 - \bar{x}_2)-(\mu_1 - \mu_2)}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$
$$t=\frac{(44.98889 - 45.14222)-(0)}{ 0.7947528} = -0.1929279$$
What is the chance of observing the t-statistic -0.193 on the t-distribution 
with the appropriate degrees of freedom?

To answer this, we would need to calculate the degrees of freedom using that 
crazy formula. We won't do this. Instead, we will ask R to do the test for us 
(and verify that our calculated t-statistic matches R's test)

### t.test in R

Pay attention to the arguments specified by `t.test`. The first argument is the 
weight data for the commercial chickens and the second argument is the weight 
data for the transgenic chickens.

```{r, echo=FALSE}
commercial_weight <- chicken_data %>% filter(type == "commercial") %>% pull(weight)
transgenic_weight <- chicken_data %>% filter(type == "transgenic") %>% pull(weight)
```

```{r}
t.test(commercial_weight, transgenic_weight, alternative = "two.sided")
```

### Robustness of the two-sample t procedures

- These procedures are more robust than the one-sample t procedures, especially
if the data are skewed.
- When the sizes of the two samples are equal and the two populations being 
compared have similar shapes, the t procedures will work well for sample sizes
as small as $n_1=n_2=5$.
- When the two populations have different shapes, larger samples are needed 
(e.g., one skewed left and the other skewed right).

## CI for two sample t-test

### Confidence intervals for the two-sample t-test
For a one sample t-test the CI looked like this:

$$\bar{x}\pm t^*\frac{s}{\sqrt{n}}$$

When we have two samples it will look like this:

$$(\bar{x_1}-\bar{x_2}) \pm t^*\sqrt{\frac{s_1^2}{n_1}+{\frac{s_2^2}{n_2}}}$$ 

where $t^*$ is the critical value with area C between $-t^*$ and $t^*$ under the $t$
density curve with the appropriate degrees of freedom.

## Non-Independent T

### Flavors of T  

So far we've been talking about independent outcomes.  Now let's extend our t-testing framework to consider what happens when those samples are NOT independent.


###Example:  Weight by gender
Imagine for example that we want to show that weight is different among males and females in the United States. Imagine we have data from 100 randomly sampled males and 100 randomly sampled females in the United States. 

We would test the null hypothesis that there is no difference between the mean weight of men and women in the united states

$$\bar X_{(group_{a})} = \bar X_{(group_{b})}$$

###Example:  Weight by gender
Would we consider these samples independent?


#standard two-sample t-test

###Two sample t-test
We want to compare the mean weight for each group, and use the standard error of the weights of these groups to calculate a t-test.  This helps us to understand if the difference in the means is larger than we might see due to the variability of the weights in our observations.  

we would calculate 

$$t=\frac{x_{groupa}-x_{groupb}}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$


and compare this to a t-distribution at our chosen critical point with appropriate degrees of freedom

###Two sample t-test 
To illustrate this example, I have simulated data for males and females using the mean and standard deviation of weights in the United States taken from the CDC NHANES data

```{r indwt, echo=FALSE}
# Read CSV into R
weights <- read.csv(file="weights.csv", header=TRUE, sep=",")
```
```{r, echo=TRUE}
weights %>% group_by(sex) %>% summarise(sample_mean = mean(weight1), 
                                                sample_sd = sd(weight1), 
                                                length = length(weight1))
```

###Two sample t-test
I can overlay the histograms for these data with this code:

ggplot(weights,aes(x=weight1)) + 

    geom_histogram(data=subset(weights,sex == 'M'),binwidth=5,fill="dark green", col="green") +

    geom_histogram(data=subset(weights,sex == 'F'),binwidth=5,fill = "blue", col="black") +

  theme_minimal(base_size = 15)

Notice that I am using two geom_histogram statements to lay the histograms on top of one another rather than using a "fill" statement in one geom_histogram.  

###Two sample t-test  
```{r ingraph,echo=FALSE, out.width="80%"}
ggplot(weights,aes(x=weight1)) + 
    geom_histogram(data=subset(weights,sex == 'M'),binwidth=5,fill="dark green", col="green") +
    geom_histogram(data=subset(weights,sex == 'F'),binwidth=5,fill = "blue", col="black") +
  theme_minimal(base_size = 15)
```

###Two sample t-test 
And a Student's T test will show that this difference is statistically significant - notice the syntax here
```{r indwt2, echo=TRUE}
t.test(weights$weight1~weights$sex, alternative="two.sided")
```

###Independent vs non-independent samples
In this example we have measured randomly selected males and females and we have no reason to believe their measurements are correlated. So a two-sample simple t-test is a reasonable approach.

What happens if we imagine that these 200 individuals are all invited to participate in a weight loss trial. 

We have their baseline weight, and after 6 months of participation in the trial they are weighted again.

What would we assume about the independence of our measures now?

###Independent vs. non-independent samples
Using r to graph the post-trial weight against the pre-trial weight we can see that these are correlated

```{r wtscatter, echo=FALSE}
plot(weights$weight1,weights$weight2)
```

###Independent vs. non-independent samples
For each individual in this study, we will will compare their weight after 6 months in the trial to their weight at baseline. Now we have broken our assumption (needed for the Student's t-test) that the measurements in the two groups (pre and post) are independent of each other. 

We would expect that each person's weight at 6 month follow up will be closely related to their own weight at baseline. We would also expect that the variation in weight within one person will be much less than the variation in weight between people. 

In this case, because I have simulated the data, I know that this hypothetical weight loss program results in an average weight loss of 5 pounds with a standard deviation of 5 pounds. 


###Independent vs. non-independent samples
Let's take a look at what happens when we use this Student's T test to compare weights before and after the intervention without taking into account the relationship of these measurements:

If we do not take into account the paired structure of the data, we are testing the null hypothesis
$$\bar X_{(weight pre trial)} = \bar X_{(weight post trial)}$$
and our t-test would be based on

$$t=\frac{\overline{X_{(weight pre trial)}}-\overline{X_{(weight post trial)}}
}{\sqrt{\left(\frac{S^2_{1}}{n_{1}}+\frac{S^2_{2}}{n_{2}}\right)}}$$

###Independent vs. non-independent samples
```{r indwt3, echo=TRUE}
t.test(weights$weight1, weights$weight3, data=weights)
```

###Independent vs. non-independent samples
We see that the estimated difference in weight is close to 5 pounds, but the results are not statistically significant. If we do not account for the relatedness of these measurements there is too much "noise" or variation between the measurements to see the "signal" or the true difference in means.

The solution to this problem is to look at the measurements in pairs and base our statistical testing on the variability in the difference between the pre and post intervention measures of weight. 

# Paired t-test 

### Paired t-test
In this case we are now testing the null hypothesis that the difference is 0

This is called a paired t-test.
$$t=\frac{\bar d_{(weightpost-weightpre)}} {\frac{S_d} {\sqrt{n}}}$$

### Paired t-test
```{r, echo=TRUE}
weights %>% summarise(dif_mean = mean(dif2), dif_sd = sd(dif2),
                      wt1_mean=mean(weight1),wt1_sd=sd(weight1),
                      wt3_mean=mean(weight3),wt3_sd=sd(weight3))
```

### Paired t-test
Notice the syntax here:
```{r indwt4, echo=TRUE}
t.test(weights$weight1, weights$weight3,data=weights, paired=TRUE)
```

###Paired test results
Here we see that the estimate of difference is unchanged, but the t-test is now using the standard deviation of the difference (4.85) rather than the standard deviation of weights between people at each time point(30.77 and 31.6) to determine whether this difference is statistically significant. 

With the paired test, our value of t is much higher and our results are statistically significant.

###Distribution of differences
If we graph the mean values and distribution of the difference between pre and post trial weight, and the overall weights post trial we can see that the variability is much smaller for the difference.

ggplot() +

geom_histogram(data = weights, aes(x = weight3), binwidth=5, fill="green") +

geom_histogram(data = weights, aes(x = dif2), binwidth=5, fill="blue") +

 labs( x = "Weight in kg") +

  theme_minimal(base_size = 15)

###Distribution of differences

```{r ingraph2,echo=FALSE, out.width="80%"}
ggplot() +
geom_histogram(data = weights, aes(x = weight3), binwidth=5, fill="green") +
geom_histogram(data = weights, aes(x = dif2), binwidth=5, fill="blue") +
 labs( x = "Weight in kg") +
  theme_minimal(base_size = 15)
```

## t-test: More juice per squeeze?


### t-test: More juice per squeeze?
When we have 

- The standard error was much lower using the paired test. Why?
- Only variation within a subject was used to calculate the SE of the mean difference
- there was much less variation within a subject than between subjects


### The Statistical Method {.emphasized}

**P**roblem<br>

**Plan**

**D**ata<br>

**A**nalysis<br>

**C**onclusion

### Plan, a.k.a. experimental design

- Once the **problem** has been stated, the next step is to determine a **plan** to best answer the question.
One of the tenets of design is to maximize <b class = "greentext">**efficiency**</b>. 
- When data are paired a paired test greatly maximizes the efficiency by removing the noise introduced by between-subject variability.

### When is a paired design the appropriate design?

 - Studies with multiple measures on the same units of observation
 - Studies with inherently related observations 
 - Studies that match units of observation to reduce variability


### Studies with mutliple measures on the same units
Cross -over or before and after studies - in our weigh-loss example we were looking at measures before and after participation...

- When "the treatment alleviates a condition rather than affects a cure." (Hills and Armitrage, 1979)
- The effect of treatment is short-term. After $x$ amount of time, participants return to baseline.
- The $x$ above refers to the <b class = "greentext">**wash-out**</b> period. Before applying the second treatment, participants should have enough time to reach their baseline level. Otherwise there may be a <b class = "greentext">**carry over**</b> effect.

### Studies with mutliple measures on the same units
Considerations for before/after or cross-over studies
- The time between the alternative treatments isn't so long as to introduce confounding by other factors.
- For example, if you waited a year between applying treatments, other things may have changed in the world
or in a person's life that affects the outcome.
- Thus, there is a balance between waiting too long or not waiting long enough.

If we wanted to look at changes in individual related to a treatment what other type of design might we consider?

###Inherently related observations
 - Matched body parts 
 - Studies in identical twins
 - Studies of diet or health behaviors in couples or family members 
 
### Studies that match to reduce variability
- Matched communities

## Example small study of diet


### Cholestorol measurements following two alternative diets - 

Suppose you received the following graphic illustrating cholesterol measurements following two alternate diets. What do you think about these data? 

```{r make-dataset, echo=F, message=F, warning=F, out.width="80%"}
# Eleven study participants were randomized to each diet, and you're tasked with 
# figuring out whether there is a difference between the diet's average cholesterol levels.

library(tidyverse)
library(RColorBrewer)

set.seed(123)
trt_a <- c(155, 180, 190, 192, 203, 201, 207, 208, 217, 228, 237)

# Model parameters: You can play around with these to see how they affect your analysis.
signal <- 7 # note: this is the true underlying mean difference we will estimate
noise <- 4  # note: 4/sqrt(n) is the true SE of the mean = 1.21, where n = length(trt_a) = 11

trt_b <- trt_a + rnorm(length(trt_a), signal, noise)

chol_dat <- data.frame("A" = trt_a, "B" = trt_b, "id" = 1:length(trt_a))

chol_dat_long <- chol_dat %>% gather(A, B, key = "diet", value = "cholesterol")
```

```{r, out.width="60%", echo=F, fig.align='center'}
ggplot(chol_dat_long, aes(diet, cholesterol)) + 
  geom_jitter(width = 0.03, pch = 21, size = 5, fill = "#2ca25f", alpha = 0.8) + 
  labs(y = "Cholesterol", x = "Diet") +
  theme_minimal(base_size = 15)
```

### Cholestorol measurements following two alternative diets - 

```{r, out.width="60%", echo=F, fig.align='center'}
means <- chol_dat_long %>% 
  group_by(diet) %>% 
  summarise(mean = round(mean(cholesterol), 1),
            median = round(median(cholesterol), 1))

ggplot(chol_dat_long, aes(diet, cholesterol)) + 
  geom_boxplot(fill = "transparent") +
  geom_jitter(width = 0.03, pch = 21, size = 5, fill = "#2ca25f", alpha = 0.8) + 
  labs(y = "Cholesterol", x = "Diet") +
  geom_text(data = means, aes(x = diet, y = 260, label = paste0("Mean: ", mean)), size = 5) +
  theme_minimal(base_size = 15)
```

- What do you notice about the variability between participants under each diet?
- What is the mean difference? 

```{r, echo=F, eval=F}
#There is a lot of between subject variability. This variation has nothing to do with the treatments.
#The mean difference is 7.8 units.
```

### Cholestorol measurements following two alternative diets - 

An independent t-test reveals no evidence against the null hypothesis of no difference between the diets:

```{r, out.width="80%", echo=F, fig.align='center'}
indep_t <- t.test(x = chol_dat %>% pull(A), 
                       y = chol_dat %>% pull(B), mu = 0, 
                       alt = "two.sided", paired = F, 
                       conf.level = 0.95)

indep_t
```

### Better visualization for a very small study

Now, what do you notice about the paired data?

```{r, out.width="80%", echo=F, fig.align='center'}
chol_dat_long <- chol_dat_long %>% mutate(diet_n = ifelse(diet == "A", 1, 2))
means <- means %>% mutate(diet_n = ifelse(diet == "A", 1, 2))

ggplot(chol_dat_long, aes(diet_n, cholesterol)) + 
  geom_line(aes(group = id), lty = 3, lwd = 1) +
  geom_point(aes(fill = as.factor(id)), pch = 21, size = 5, alpha = 0.8) + 
  scale_fill_brewer(palette = "Spectral") +
  labs(y = "Cholesterol", x = "Diet") +
  geom_text(data = means, aes(x = diet_n, y = 260, label = paste0("Mean: ", mean)), size = 5) +
  theme_minimal() +
  guides(fill = guide_legend("ID")) +
  scale_x_continuous(breaks = c(1, 2), labels = c("A", "B"), limits = c(0.5, 2.5)) +
  theme_minimal(base_size = 15)
  theme(panel.grid.minor.x = element_blank())
```

```{r, echo=F, eval=F}
# There is a consistent increase in cholesterol under Diet B. 
# We also know that the mean of the individual differences is equal to the difference
# of the means --> the average of the individual differences is also 209.4-201.6 = 7.8
# So what is different about the test? It's the standard error. 
```

### apply a paired t-test


- The observed value of the test statistic is: 
$t = \frac{\bar{x}_d-0}{s_d/\sqrt{n}}$

- It can be compared to a critical value from the t distribution with $n-1$ degrees of freedom

### Calculate the test statistic, p-value, and 95% confidence interval {.demphasize}

- First let's have a look at the dataset as is:

```{r peak-data} 
head(chol_dat)
```

### Calculate the test statistic, p-value, and 95% confidence interval {.demphasize}

- We can use functions from the library `dplyr` to calculate the test statistic
- Use `mutate` to calculate each participant's difference:

```{r mutate-diff, echo=TRUE}
chol_dat <- chol_dat %>%mutate(diff = B - A)
head(chol_dat)
```

### Calculate the test statistic, p-value, and 95% confidence interval

- Then use `summarize` to calculate the mean difference ($\hat{\mu}_d$), its
standard error ($\hat{s}_d/\sqrt{n}$), and the observed t-statistic:

```{r calc-t-stat, echo=TRUE}
summary_stats <- chol_dat %>% 
  summarize(mean_diff = mean(diff),  # mean difference
            std_err_diff = sd(diff)/sqrt(n()),  # SE of the mean 
            t_stat = mean_diff/std_err_diff)   # test statistic
summary_stats
```

### Calculate the test statistic, p-value, and 95% confidence interval {.tiny}

What is the <b class = "greentext">probability</b> of observing a t-stat $\geq$ `r round(summary_stats$t_stat, 2)` or $\leq$ `r round(-summary_stats$t_stat, 2)` using the `pt` command.

```{r graph-t, out.width="80%", echo=F, fig.align='center'}
##You do not need to know how to do this
ggplot(data.frame(x = rt(100000, 10)), aes(x = x)) + 
  stat_function(fun = dt, args = list(df = 10)) + 
  scale_x_continuous(limits = c(-9, 9), 
                     breaks = c(-8, -4, 0, 4, 8), 
                     labels = c(-8, -4, 0, 4, 8))  +
  geom_vline(xintercept = c(-summary_stats$t_stat, summary_stats$t_stat), lty = 3) +
  geom_area(stat = "function", fun = dt, args = list(df = 10), fill = "blue", xlim = c(-5, -summary_stats$t_stat)) +
  geom_area(stat = "function", fun = dt, args = list(df = 10), fill = "blue", xlim = c(summary_stats$t_stat, 5)) + 
  labs(x = "t distribution") + 
  theme(panel.grid.minor = element_blank()) 
```

```{r calc-pval}
pt(q = summary_stats %>% pull(t_stat), lower.tail = F, df = 10) * 2
```

### Calculate the test statistic, p-value, and 95% confidence interval {.demphasized}

- To calculate the 95% confidence interval, we need to know the quantile of the 
t distribution such that 2.5% of the data lies above or below it.

- Ask R: What is the <b class = "greentext">quantile</b> such that 97.5% of the t-distribution is below it on 10 degrees of freedom using the `qt` command.

### Calculate the test statistic, p-value, and 95% confidence interval {.demphasized}
```{r calc-CI, echo=TRUE}
q <- qt(p = 0.975, lower.tail = T, df = 10)
q
ucl <- summary_stats %>% pull(mean_diff) + (q * summary_stats %>% pull(std_err_diff))   
lcl <- summary_stats %>% pull(mean_diff) - (q * summary_stats %>% pull(std_err_diff))  
c(lcl, ucl)
```

The confidence interval is (`r lcl`, `r ucl`).

### Calculate the test statistic, p-value, and 95% confidence interval {.demphasized}

- Or, have R do the work for you! Just be sure to specify that `paired = T`.
```{r lazy-way}
paired_t <- t.test(chol_dat %>% pull(B), chol_dat %>% pull(A), 
                   alternative = "two.sided", mu = 0, paired = T)
```

### Calculate the test statistic, p-value, and 95% confidence interval {.demphasized}
```{r lazy2}
paired_t
```

### Compare the outputs from the independent and paired tests

|                   | Independent      |Paired                   | 
|-------------------|------------------|-------------------------|
|  T statistic      |         -0.78557 |               6.6033    |
| df                |         19.976   |           10            |
|pvalue             |         0.4413   |           6.053e-05     |
|mean               | 201.67 vs  209.35|       7.72              |
|95% CI             | -28.21 to 12.78  |  5.11 to 10.32          |
|SE                 | 9.823            |  1.169                  |

- What is the same?
- What is different?

### coding notes

A one sample t- test will take the form:

t.test(x = **x variable**, alternative = **greater, less or two.sided**, mu = **null hypothsis value**)

A two sample t-test will take the form:

t.test(**first sample data**, **second sample data**, alternative = **greater, less or two.sided**)

A paired t-test will take the form:

t.test(**first data points**, **second datapoints**, alternative = **greater, less or two.sided**, paired=TRUE)


